---
title: "Working with Weather and Climate Data in R"
author: "Loïc Henry"
date: today
format: html
execute:
  echo: true
  warning: false
  message: false
---

# Introduction

Understanding and analyzing weather data is central to many applications in economics, environmental sciences, and policy evaluation.  
This document introduces the main **data sources** and demonstrates an **R-based workflow** to download, explore, and aggregate weather data (using ERA5 reanalysis as an example).

---

## Choosing Weather Data: data type and source

### Weather station data
- Provide **accurate** weather data for specific locations.  
- Have **missing observations**, since stations appear/disappear over time.  
- Possible to interpolate missing data (e.g., inverse distance weighting), but this can introduce measurement errors and bias.  
- Example: [NOAA Global Daily Weather Station Data](https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ncdc:C00861/html)

### Gridded weather datasets
- Provide a **uniform** weather record across space and time.  
- Constructed from station data + statistical interpolation and/or **reanalysis models**. Be aware that this reanalysis can sometimes introduce local biases for some weather variables.
- Several freely available sources:  
  - [PRISM](https://prism.oregonstate.edu/) — US, daily, high resolution (4km).  
  - [ERA5](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=overview) — global, hourly, ~25km resolution. We will use this one in the following, but at the monthly resolution, zooming on Europe, to limit the size of the data.
  - [CRU](https://crudata.uea.ac.uk/cru/data/hrg/) — global, monthly, ~56km resolution.  

---

## Setup: Importing ERA5 Data

To work with ERA5 data, you need to:

1. Register at ECMWF: <https://www.ecmwf.int/>  
2. Get your API keys: <https://cds.climate.copernicus.eu/how-to-api>  
3. Save your key securely — **never share it**. 
  * Save it in a R Script in your project, named `ERA5_APIKey.R`, and add this file to your `.gitignore` if you use Git.
4. Go to [ERA5-Land Monthly Statistics](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-land-monthly-means?tab=download), accept the terms, select the data of your choice and copy the API request.
  * For our exercise: select "Monthly averaged reanalysis" for product type, "2m temperature" for variable, select observations corresponding to your year and month of birth, limit the subregion to Europe (72° North, -31° West, 27° South and 60° East), and select "netcdf4" for data format. Then click to unhide the API request shown in the bottom of the webpage, and copy it.
5. Use the [`ecmwfr`](https://bluegreen-labs.github.io/ecmwfr/) package in R to translate ERA5 API requests into built-in R commands from the `ecmwfr` package.  

---

# Workflow in R

We now illustrate a complete workflow for downloading, importing, exploring, and aggregating ERA5 weather data in R.

## Preamble of your scripts

First, set up all your working directories, load useful packages and then add your API key.

```{r setup}
#===============================================================================
# Preamble: setup the folders and load packages ------
#===============================================================================

# Clean memory 
rm(list=ls())
gc()

# Load packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse, terra, maps, here, ncdf4, raster, climate, devtools, 
  sf, sp, rnaturalearth, Matrix, ecmwfr
)

# Setup project directories
dir <- list()
dir$root <- here()                       # R project root 
dir$data <- here(dir$root, "data")       # processed data
dir$source <- here(dir$data, "source")   # raw data
dir$intermediary <- here(dir$data, "intermediary")   # raw data
dir$final <- here(dir$data, "final")     # final data
dir$code <- here(dir$root, "code")       # scripts
dir$figures <- here(dir$root, "figures") # plots
dir$tables <- here(dir$root, "tables")   # tables

# Create missing directories
lapply(dir, function(i) dir.create(i, recursive = TRUE, showWarnings = FALSE))

# Register your API key once
# Execute the script where you have stored identification key
source(here(dir$code, "ERA5_APIKey.R"))
# And then set key
ecmwfr::wf_set_key(key = key_loic)

```

## Downloading one batch of data
Let's import the temperature data for your month of birth in Europe.

```{r download}

# This is the request you copied from the ERA5 webpage
# dataset = "reanalysis-era5-land-monthly-means"
# request = {
#     "product_type": ["monthly_averaged_reanalysis"],
#     "variable": ["2m_temperature"],
#     "year": ["1993"],
#     "month": ["07"],
#     "time": ["00:00"],
#     "data_format": "netcdf",
#     "download_format": "unarchived",
#     "area": [72, -31, 27, 60]
# }
# Translate it into a list for ecmwfr and get this below:

request <- list(
  dataset_short_name = "reanalysis-era5-land-monthly-means",
  product_type = "monthly_averaged_reanalysis",
  variable = "2m_temperature",
  year = "1993",
  month = "07",
  time = "00:00",
  data_format = "netcdf",
  download_format = "unarchived",
  area = c(72, -31, 27, 60),
  target = "TMPFILE.nc"  # name of the output file
)


# Download using commands from ecmwfr (takes ~1 min for one month of Europe data)
file <- ecmwfr::wf_request(
  request  = request,
  transfer = TRUE,
  path     = here(dir$source)
)

```


## Import the data

We here see three different ways of manipulating netCDF data in R. The `ncdf4` package is the most basic one, but requires more coding to manipulate the data. The `terra` and `raster` packages provide more user-friendly functions to manipulate raster data, and are more similar to GIS software. The `raster` package is older and more widely used, but the `terra` package is its successor and is being actively developed.

```{r import-data}
# Open with ncdf4
nc_temp <- ncdf4::nc_open(file)

# With terra
terra_temp <- terra::rast(file)
terra_temp

# With raster
raster_brick <- raster::brick(file)
raster_brick
```

We can see that our spatial grid is made of 451 rows and 911 columns, which gives a total of 410,861 grid cells covering Europe. The data contains one layer corresponding to the average temperature at 2m above ground for your month of birth. If you had more variable associated to this same spatial grid (e.g., precipitation, wind speed, etc.), or if you had data for multiple months/years, you would have more layers in your raster object.


## Inspect and Explore the data

Raster objects in R are indexed as [row, column, layer]. You can use the `terra` or `raster` packages to explore and visualize the data.

```{r explore-object, echo=FALSE}
# inspect the raster object
# Extract the first row
head(terra_temp[1,,])
dim(terra_temp[1,,])

# Extract the first cell
terra_temp[1,1,]
dim(terra_temp[1,1,])

```

You can thus easily extract the variables in a raster to store them in an array (equivalent to a matrix object in R), which will be very often useful to make derivations or to aggregate the data spatially.

```{r extract-variable}

# Extract temperature values in an array "row x col x time"
temp_array <- terra::as.array(terra_temp)
str(temp_array)
dim(temp_array) # 3D array: row x col x time

# Extract temperature values in a matrix "cell x time"
temp_matrix <- terra::as.matrix(terra_temp)
str(temp_array)
dim(temp_array) # 2D array: cell x time

```

You can also easily transform your raster in a dataframe using the `as.data.frame()` function from the `terra` or `raster` packages.

```{r raster-to-dataframe}
# Transform raster to dataframe
temp_df <- as.data.frame(terra_temp, xy = TRUE)
str(temp_df)

# Rename temperature variable
names(temp_df)[3] <- "t2m"

```


You can also visualize one layer of your raster using the built-in R command `plot()`, or use the `ggplot2` package for more advanced visualizations.

```{r visualize-raster}

# Visualize using the plot command
plot(terra_temp[[1]], main = "ERA-5 Reanalysis Demo (Temperature)")


# Visualize using commands from ggplot2 library
# Once your raster data is transformed in a data.frame object
ggplot(temp_df) +
  geom_raster(aes(x = x, y = y, fill = t2m)) +
  scale_fill_viridis_c() +
  coord_fixed() +
  labs(title = "ERA-5 Reanalysis Demo (Temperature)")

```

## Aggregate spatially (example: EU Nuts3)

In many situations, you must aggregate gridded weather/climate data to match the geographic and temporal scale of socio-economic outcomes (e.g., departments, counties, municipalities). Let's see one way to do it in R.

### Load your geographic boundaries shape file

We first download the geographical boundaries of the EU NUTS-2 regions on this [webpage](https://ec.europa.eu/eurostat/web/gisco/geodata/statistical-units/territorial-units-statistics). Then, unzip the downloaded file, and store it into your `source` data folder.


We will load the shapefile in R using the `sf` package.

```{r aggregate-spatially}

# Find the shp file
shp_file <- list.files(here(dir$source), pattern = "*.shp", full.names = TRUE, recursive = T)

# Load the shapefile with sf package
nuts3_sf <- sf::st_read(shp_file)

```

This object is a shapefile: it is a specific object in R, not so far from the structure of a `data.frame` object, except that it has a special column containing the geometry of each spatial unit (here, the NUTS3 regions). 


```{r str-shapefile}

# Inspect the shapefile
str(nuts3_sf)
head(nuts3_sf)

```


You can visualize it using the `ggplot2` package.

```{r visualize-shapefile}
# Visualize the shapefile
ggplot(nuts3_sf) +
  geom_sf(fill = "lightgrey", color = "black", alpha = 0.75) +
  labs(title = "NUTS3 Regions in Europe")

```

If we want to keep the regions within our specific delimited area (Europe), we can crop the shapefile.

```{r crop-shapefile}
# Crop the shapefile to keep only regions within our area of interest
crs_nuts3 <- st_crs(nuts3_sf)
# Define your lat/lon bounding box
bbox_lonlat <- st_bbox(c(xmin = -31, xmax = 60, ymin = 27, ymax = 72), 
                       crs = st_crs(4326))  # WGS84 lat/lon

# Convert bbox to an sf object, then transform to match your data's CRS
bbox_transformed <- st_as_sfc(bbox_lonlat) %>%
  st_transform(crs_nuts3) %>%
  st_bbox()

# Now crop with the transformed bbox
crop_nuts3_sf <- st_crop(nuts3_sf, bbox_transformed)

# Then, only keep nuts-3 levels
crop_nuts3_sf <- crop_nuts3_sf %>% 
  filter(LEVL_CODE == "3")

```


Check by vizualising that you have indeed now a smaller shapefile:
```{r visualize-cropped-shapefile}
# Visualize the cropped shapefile
ggplot(crop_nuts3_sf) +
  geom_sf(fill = "lightgrey", color = "black", alpha = 0.75) +
  labs(title = "Cropped NUTS3 Regions in Europe")
```


Then, before aggregating your weather data at the NUTS3-level, we must spatially merge them. In particular, we want our NUTS3 shape file to have the exact same geographical coordinate system as the ones in out weather grid. For instance, if we currently layered our two raster and sf objects, we would see that they do not overlap perfectly.

```{r check-overlap1}

# Quick visualization to check overlap using ggplot2
ggplot() +
  geom_sf(data = crop_nuts3_sf, fill = "lightgrey", color = "black", alpha = 0.025) +
  geom_raster(data = temp_df, aes(x = x, y = y, fill = t2m)) +
  scale_fill_viridis_c() +
  labs(title = "Check Overlap between NUTS3 and ERA5 Data")

```

There is no overlap.

We know that ERA5 data uses a regular latitude-longitude grid in the WGS84 coordinate system, which corresponds to EPSG:4326. Let's transform the NUTS3 shapefile to this CRS.


```{r transform-crs-nuts3}

# First, inform R about the coordinate system and extent of the raster
crs(terra_temp) <- "EPSG:4326"

# Second, Fix the extent based on your actual data coverage
ext(terra_temp) <- c(-31, 60, 27, 72)  # xmin, xmax, ymin, ymax

# Reproject nuts3 to EPSG:4326

nuts3_sp <- st_transform(crop_nuts3_sf, crs = crs(terra_temp))

```

Check the overlap:

```{r check-overlap2}

# Plot together to verify alignment
plot(terra_temp, main = "ERA5 with Shapefile")
plot(st_geometry(nuts3_sp), add = TRUE, border = "red", lwd = 2)

```


### Construct the aggregation matrix

We now construct a projection matrix $P$ such that:

$$ A = P \times G $$

where:
- $G$ is the matrix of gridded climate data (cells × time),
- $A$ is the aggregated matrix (Nuts-3 × time).

```{r projection-matrix}

# Prepare raster for aggregation
id <- terra_temp # rename it just for convenience
# need to have an id for each cells to the raster
id_cells <- terra::setValues(terra::rast(id), 1:terra::ncell(id))

# Identify in which nuts3 region falls each cell using extract command from terra package (CAN TAKE UP TO 10 MINUTES!)
info <- terra::extract(
  x = c(id_cells,id), 
  y = nuts3_sp,
  cellnumbers = TRUE,
  weights = TRUE,
  ID= TRUE
 )

# info is a data.frame with: ID (polygon), cell, weight, and raster values
head(info)
str(info)

# rename variables
names(info)[3] <- "t2m"
names(info)[2] <- "cell"
names(info)[1] <- "poly_id"

# Remove any na values
info <- na.omit(info)

# Check number of unique nuts3 regions where there are ERA5 raster cells
length(unique(info$poly_id))  # should be less or equal than total number of nuts3 regions in nuts3_sp

# Normalize weights within each polygon: to be the sum of weights (area) of each cell falling in the polygon
info <- do.call(rbind, lapply(split(info, info$poly_id), function(df) {
  df$w_norm <- df$weight / sum(df$weight, na.rm = TRUE)
  df
}))

# Check that we have well added all cell areas / or weights, within each polygons
check_weights <- info %>%
  group_by(poly_id) %>%
  summarize(
    N_cells = n(),
    area = sum(weight, na.rm = TRUE), # this is the area of the nuts 3 region
    total_weight = sum(w_norm, na.rm = TRUE) # this should be equal to 1
    )
unique(check_weights$total_weight)

# Ensure indices are integers
info$cell <- as.integer(info$cell)
info$poly_id <- as.integer(factor(info$poly_id))

# Build sparse matrix safely
P_weight <- Matrix::sparseMatrix(
  i = info$cell,
  j = info$poly_id,
  x = info$w_norm,
  dims = c(terra::ncell(id), length(unique(info$poly_id)))
)

# Raster values as a matrix (each layer = column)
G <- as.matrix(values(terra_temp))

# Compute polygon-level weighted mean (aggregate)
A_weight <- t(P_weight) %*% G

# Convert to data frame
nuts3_temp <- as.data.frame(as.matrix(A_weight))
nuts3_temp$poly_id <- unique(info$poly_id)

" Have a look at data at nuts-3 lebv"
head(nuts3_temp)

```


## Visualize aggregated results

Now, to check that things are aggregated correctly, we can overlap our raster data for temperature with the polygon raster data.

```{r maps-spataggreg}


```


# Next steps to go further


## Get long series of weather data

The downloading steps would need to be reiterated on multiple months to get several time periods of weather data. 

Explain here how you could generalize the code to download data on a parametrized period.

```{r download-longdata}

```

## Build weather metrics

We hypothesize that we have now monthly data over several years. We would then need to aggregate it at the yearly level. We thus need to compute several statistics at the cell level giving the yearly mean temperature for instance. 

Explain here how you would compute this weather statistics for each cell and each year:

```{r yearly-weather}

```

## Spatially aggregate the weather metrics

Now we have yearly weather, we would like to aggregate it at the polygon level. We have constructed our transformation matrix: when you multiply any raster with the same resolution as the ERA5 data by this matrix, you get spatially averaged data at the NUTS3-polygon level. 

Now, how can you reiterate this process for all years of weather data?

```{r nuts3-weather}

```

## Get economic data at the NUTS3 level

If we want now to estimage the effect of temperature on economic activity in the EU, we would need economic data. In R, we can use the `eurostat` package, which allows to download economic data from the Eurostat API. You can proceed as follows:

```{r download-eurostat}

# install.packages("eurostat")  
library(eurostat)

# Search for GDP datasets at NUTS level
search_results <- search_eurostat("GDP NUTS", type = "dataset")
View(search_results)

# The main dataset for regional GDP is "nama_10r_3gdp"
# This contains GDP data at NUTS2 and NUTS3 levels

# Download the GDP data
gdp_data <- get_eurostat("nama_10r_3gdp", time_format = "num")

# View the structure of the data
str(gdp_data)
head(gdp_data)

# Filter for NUTS3 level data only
# NUTS3 codes have 5 characters (e.g., "DE211", "FR101")
gdp_nuts3 <- gdp_data %>%
  filter(nchar(as.character(geo)) == 5)

# Filter for specific unit (e.g., current prices in millions of euros)
# Common units:
# - MIO_EUR: Million euros
# - MIO_PPS: Million purchasing power standards

gdp_nuts3_current <- gdp_nuts3 %>%
  filter(unit == "MIO_EUR")

# View available years
unique(gdp_nuts3_current$time)

# Filter for years where you have weather data 
gdp_nuts3_recent <- gdp_nuts3_current %>%
  filter(time >= 2000 & time <= 2024)

# View the result
head(gdp_nuts3_recent, 20)

# Plot some of the GDP series to have a look
# Select randomly 10 NUTS3 regions


# plot their time series



```


