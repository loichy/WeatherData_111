---
title: "Working with Weather and Climate Data in R"
author: "Loïc Henry"
date: today
format: html
execute:
  echo: true
  warning: false
  message: false
---

# Introduction

Understanding and analyzing weather data is central to many applications in economics, environmental sciences, and policy evaluation.  
This document introduces the main **data sources** and demonstrates an **R-based workflow** to download, explore, and aggregate weather data (using ERA5 reanalysis as an example).

---

## Choosing Weather Data: data type and source

### Weather station data
- Provide **accurate** weather data for specific locations.  
- Have **missing observations**, since stations appear/disappear over time.  
- Possible to interpolate missing data (e.g., inverse distance weighting), but this can introduce measurement errors and bias.  
- Example: [NOAA Global Daily Weather Station Data](https://www.ncei.noaa.gov/metadata/geoportal/rest/metadata/item/gov.noaa.ncdc:C00861/html)

### Gridded weather datasets
- Provide a **uniform** weather record across space and time.  
- Constructed from station data + statistical interpolation and/or **reanalysis models**. Be aware that this reanalysis can sometimes introduce local biases for some weather variables.
- Several freely available sources:  
  - [PRISM](https://prism.oregonstate.edu/) — US, daily, high resolution (4km).  
  - [ERA5](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-single-levels?tab=overview) — global, hourly, ~25km resolution. We will use this one in the following, but at the monthly resolution, zooming on Europe, to limit the size of the data.
  - [CRU](https://crudata.uea.ac.uk/cru/data/hrg/) — global, monthly, ~56km resolution.  

---

## Setup: Importing ERA5 Data

To work with ERA5 data, you need to:

1. Register at ECMWF: <https://www.ecmwf.int/>  
2. Get your API keys: <https://cds.climate.copernicus.eu/how-to-api>  
3. Save your key securely — **never share it**. 
  * Save it in a R Script in your project, named `ERA5_APIKey.R`, and add this file to your `.gitignore` if you use Git.
4. Go to [ERA5-Land Monthly Statistics](https://cds.climate.copernicus.eu/datasets/reanalysis-era5-land-monthly-means?tab=download), accept the terms, select the data of your choice and copy the API request.
  * For our exercise: select "Monthly averaged reanalysis" for product type, "2m temperature" for variable, select observations corresponding to your year and month of birth, limit the subregion to Europe (72° North, -31° West, 27° South and 60° East), and select "netcdf4" for data format. Then click to unhide the API request shown in the bottom of the webpage, and copy it.
5. Use the [`ecmwfr`](https://bluegreen-labs.github.io/ecmwfr/) package in R to translate ERA5 API requests into built-in R commands from the `ecmwfr` package.  

---

# Workflow in R

We now illustrate a complete workflow for downloading, importing, exploring, and aggregating ERA5 weather data in R.

## Preamble of your scripts

First, set up all your working directories, load useful packages and then add your API key.

```{r setup}
#===============================================================================
# Preamble: setup the folders and load packages ------
#===============================================================================

# Clean memory 
rm(list=ls())
gc()

# Load packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse, terra, maps, here, ncdf4, raster, climate, devtools, 
  sf, sp, rnaturalearth, Matrix, ecmwfr
)

# Setup project directories
dir <- list()
dir$root <- here()                       # R project root 
dir$data <- here(dir$root, "data")       # processed data
dir$source <- here(dir$data, "source")   # raw data
dir$intermediary <- here(dir$data, "intermediary")   # raw data
dir$final <- here(dir$data, "final")     # final data
dir$code <- here(dir$root, "code")       # scripts
dir$figures <- here(dir$root, "figures") # plots
dir$tables <- here(dir$root, "tables")   # tables

# Create missing directories
lapply(dir, function(i) dir.create(i, recursive = TRUE, showWarnings = FALSE))

# Register your API key once
# Execute the script where you have stored identification key
source(here(dir$code, "ERA5_APIKey.R"))
# And then set key
ecmwfr::wf_set_key(key = key_loic)

```

## Downloading one batch of data
Let's import the temperature data for your month of birth in Europe.

```{r download}

# This is the request you copied from the ERA5 webpage
# dataset = "reanalysis-era5-land-monthly-means"
# request = {
#     "product_type": ["monthly_averaged_reanalysis"],
#     "variable": ["2m_temperature"],
#     "year": ["1993"],
#     "month": ["07"],
#     "time": ["00:00"],
#     "data_format": "netcdf",
#     "download_format": "unarchived",
#     "area": [72, -31, 27, 60]
# }
# Translate it into a list for ecmwfr and get this below:

request <- list(
  dataset_short_name = "reanalysis-era5-land-monthly-means",
  product_type = "monthly_averaged_reanalysis",
  variable = "2m_temperature",
  year = "1993",
  month = "07",
  time = "00:00",
  data_format = "netcdf",
  download_format = "unarchived",
  area = c(72, -31, 27, 60),
  target = "TMPFILE"
)


# Download using commands from ecmwfr (takes ~1 min for one month of Europe data)
file <- ecmwfr::wf_request(
  request  = request,
  transfer = TRUE,
  path     = here(dir$source)
)

```


## Import the data

We here see three different ways of manipulating netCDF data in R. The `ncdf4` package is the most basic one, but requires more coding to manipulate the data. The `terra` and `raster` packages provide more user-friendly functions to manipulate raster data, and are more similar to GIS software. The `raster` package is older and more widely used, but the `terra` package is its successor and is being actively developed.

```{r import-data}
# Open with ncdf4
nc_temp <- ncdf4::nc_open(file)

# With terra
terra_temp <- terra::rast(file)
terra_temp

# With raster
raster_brick <- raster::brick(file)
raster_brick
```

We can see that our spatial grid is made of 451 rows and 911 columns, which gives a total of 410,761 grid cells covering Europe. The data contains one layer corresponding to the average temperature at 2m above ground for your month of birth. If you had more variable associated to this same spatial grid (e.g., precipitation, wind speed, etc.), or if you had data for multiple months/years, you would have more layers in your raster object.


## Inspect and Explore the data

Raster objects in R are indexed as [row, column, layer]. You can use the `terra` or `raster` packages to explore and visualize the data.

```{r explore-object, echo=FALSE}
# inspect the raster object
# Extract the first row
terra_temp[1,,]
dim(terra_temp[1,,])

# Extract the first cell
terra_temp[1,1,]
dim(terra_temp[1,1,])

```

You can thus easily extract the variables in a raster to store them in an array (equivalent to a matrix object in R), which will be very often useful to make derivations or to aggregate the data spatially.

```{r extract-variable}

# Extract temperature values in an array "row x col x time"
temp_array <- terra::as.array(terra_temp)
str(temp_array)
dim(temp_array) # 3D array: row x col x time

# Extract temperature values in a matrix "cell x time"
temp_matrix <- terra::as.matrix(terra_temp)
str(temp_array)
dim(temp_array) # 2D array: cell x time

```

You can also easily transform your raster in a dataframe using the `as.data.frame()` function from the `terra` or `raster` packages.

```{r raster-to-dataframe}
# Transform raster to dataframe
temp_df <- as.data.frame(terra_temp, xy = TRUE)
str(temp_df)
```


You can also visualize one layer of your raster using the built-in R command `plot()`, or use the `ggplot2` package for more advanced visualizations.

```{r visualize-raster}

# Visualize using the plot command
plot(terra_temp[[1]], main = "ERA-5 Reanalysis Demo (Temperature)")


# Visualize using commands from ggplot2 library
# Once your raster data is transformed in a data.frame object
ggplot(temp_df) +
  geom_raster(aes(x = x, y = y, fill = TMPFILE)) +
  scale_fill_viridis_c() +
  coord_fixed() +
  labs(title = "ERA-5 Reanalysis Demo (Temperature)")

```

## Aggregate spatially (example: EU Nuts3)

In many situations, you must aggregate gridded weather/climate data to match the geographic and temporal scale of socio-economic outcomes (e.g., departments, counties, municipalities). Let's see one way to do it in R.

### Load your geographic boundaries shape file

We first download the geographical boundaries of the EU NUTS-2 regions on this [webpage](https://ec.europa.eu/eurostat/web/gisco/geodata/statistical-units/territorial-units-statistics). Then, unzip the downloaded file, and store it into your `source` data folder.


We will load the shapefile in R using the `sf` package.

```{r aggregate-spatially}

# Find the shp file
shp_file <- list.files(here(dir$source), pattern = "*.shp", full.names = TRUE, recursive = T)

# Load the shapefile with sf package
nuts3_sf <- sf::st_read(shp_file)

```

This object is a shapefile: it is a specific object in R, not so far from the structure of a `data.frame` object, except that it has a special column containing the geometry of each spatial unit (here, the NUTS3 regions). 


```{r str-shapefile}

# Inspect the shapefile
str(nuts3_sf)
head(nuts3_sf)

```


You can visualize it using the `ggplot2` package.

```{r visualize-shapefile}
# Visualize the shapefile
ggplot(nuts3_sf) +
  geom_sf(fill = "lightgrey", color = "black", alpha = 0.75) +
  labs(title = "NUTS3 Regions in Europe")

```

If we want to keep the regions within our specific delimited area (Europe), we can crop the shapefile.

```{r crop-shapefile}
# Crop the shapefile to keep only regions within our area of interest
crs_nuts3 <- st_crs(nuts3_sf)
# Define your lat/lon bounding box
bbox_lonlat <- st_bbox(c(xmin = -31, xmax = 60, ymin = 27, ymax = 72), 
                       crs = st_crs(4326))  # WGS84 lat/lon

# Convert bbox to an sf object, then transform to match your data's CRS
bbox_transformed <- st_as_sfc(bbox_lonlat) %>%
  st_transform(crs_nuts3) %>%
  st_bbox()

# Now crop with the transformed bbox
crop_nuts3_sf <- st_crop(nuts3_sf, bbox_transformed)


```


Check by vizualising that you have indeed now a smaller shapefile:
```{r visualize-cropped-shapefile}
# Visualize the cropped shapefile
ggplot(crop_nuts3_sf) +
  geom_sf(fill = "lightgrey", color = "black", alpha = 0.75) +
  labs(title = "Cropped NUTS3 Regions in Europe")
```


Then, before aggregating your weather data at the NUTS3-level, we must spatially merge them. In particular, we want our NUTS3 shape file to have the exact same geographical coordinate system as the ones in out weather grid. For instance, if we currently layered our two raster and sf objects, we would see that they do not overlap perfectly.

```{r check-overlap1}

# Quick visualization to check overlap using ggplot2
ggplot() +
  geom_sf(data = crop_nuts3_sf, fill = "lightgrey", color = "black", alpha = 0.025) +
  geom_raster(data = temp_df, aes(x = x, y = y, fill = TMPFILE)) +
  scale_fill_viridis_c() +
  labs(title = "Check Overlap between NUTS3 and ERA5 Data")

```

There is no overlap.

We know that ERA5 data uses a regular latitude-longitude grid in the WGS84 coordinate system, which corresponds to EPSG:4326. Let's transform the NUTS3 shapefile to this CRS.


```{r transform-crs-nuts3}

# First, inform R about the coordinate system and extent of the raster
crs(terra_temp) <- "EPSG:4326"

# Second, Fix the extent based on your actual data coverage
ext(terra_temp) <- c(-31, 60, 27, 72)  # xmin, xmax, ymin, ymax

# Reproject nuts3 to EPSG:4326

nuts3_sp <- st_transform(crop_nuts3_sf, crs = crs(terra_temp))

```

Check the overlap:

```{r check-overlap2}

# Plot together to verify alignment
plot(terra_temp, main = "ERA5 with Shapefile")
plot(st_geometry(nuts3_sp), add = TRUE, border = "red", lwd = 2)

```


### Construct the aggregation matrix

We now construct a projection matrix $P$ such that:

$$ A = P \times G $$

where:
- $G$ is the matrix of gridded climate data (cells × time),
- $A$ is the aggregated matrix (Nuts-3 × time).

```{r projection-matrix}

# Identify in which nuts3 region falls each cell (CAN TAKE UP TO 10 MINUTES!)
id <- terra_temp
info <- raster::extract(x = id, y = nuts3_sp, cellnumbers=TRUE)


# Build transformation matrix with weights
dinfo <- do.call("rbind", lapply(names(info), function(i) {
  df <- as.data.frame(info[[i]])
  df$w <- df$gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_2pt5_min *
          df$weight / sum(df$gpw_v4_population_count_adjusted_to_2015_unwpp_country_totals_rev11_2020_2pt5_min *
                          df$weight, na.rm = TRUE)
  df$dept.order <- match(i,names(info))
  df
}))

# Sparse projection matrix
g <- stack(raster_brick[[1]], raster_brick[[2]], raster_brick[[3]])
G <- g[]  

P_weight <- Matrix::sparseMatrix(
  i = dinfo$cell,
  j = dinfo$dept.order,
  x = dinfo$w,
  dims = c(ncell(g), length(unique(dinfo$dept.order)))
)

# Aggregated data
A_weight <- t(P_weight) %*% G
nuts3_temp <- data.frame(as.matrix(A_weight))
nuts3_temp$ame <- row.names(nuts3_temp)
```


## Visualize aggregated results

```{r maps}


```



